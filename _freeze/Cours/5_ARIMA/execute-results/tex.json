{
  "hash": "c58c3b3d2c879e05e9ea159956aa7f3c",
  "result": {
    "markdown": "---\ntitle: \"5 - Modèles ARIMA\"\n---\n\n\n\n\n### Objectifs de cette séquence\n\n- Notions sur la stationnarisation et la différenciation\n\n- Modèles (S)ARIMA\n\n\n\n### Questions de positionnement\n\nQu'est-ce qu'un processus stationnaire ?\n\\vfill\n\nTendance, cycle, saisonnalité sont-ils des processus stationnaires ?\n\\vfill\n\nQue signifie \"ARIMA\" et que reflète un tel modèle ?\n\\vfill\n\nComment se comportent les erreurs de prévision d'un modèle ARIMA ?\n\\vfill\n\nQu'est-ce qu'un SARMA ?\n\\vfill\n\nY a-t-il un lien entre ARIMA et ETS ?\n\n\n# Stationnarité et différenciation\n\n## Notion de stationnarité\n\n### Quelques définitions (1/2)\n\n*Série temporelle *: suite de variables aléatoires $(X_t)_t$ dont on observe une réalisation $(X_t(\\omega))_t$\n\nLa suite $(X_t)_t$ est appelée *processus stochastique*\n\n. . .\n\nUn processus est dit *stationnaire* lorsque la loi de $X_t$\tn'évolue pas dans le temps : distribution $\\forall s,\\,(X_t,\\dots,X_{t+s})$ indépendante du temps\n\n$\\implies$ série plus ou moins horizontale et de variance constante\n\n{{< fa arrow-circle-right >}} Notion pour faire l'inférence et construire un modèle ARIMA\n\n### Quelques définitions (2/2)\n\nStationnarité, hypothèse invérifiable {{< fa arrow-circle-right >}} en pratique processus *faiblement stationnaire* :  \n\n- les moments d'ordre 2 existent\n\n- espérance constante  \n\n- covariance entre $t$ et $t-h$ ne dépend pas du temps, mais de la distance $h$  \n    $\\implies$ variance constante\n\n\\pause\n\nExemple : un bruit blanc, i.e. : \n\n- espérance nulle  \n\n- covariance entre $t$ et $t-h$ nulle, pour tout $h\\ne0$  \n\n- variance non nulle et constante\n\n## Repérer la stationnarité\n\n### Comment identifier une série non-stationnaire (en niveau) ?\n\n- Tracer le chronogramme\n\n- Etudier l'ACF : \n\n\t- Série non-stationnaire : tend lentement vers 0 et $\\hat\\rho(1)$ souvent positif et élevé\n\t\n\t- Série stationnaire : tend rapidement vers 0\n\n### Exemple\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](5_ARIMA_files/figure-beamer/unnamed-chunk-1-1.pdf)\n:::\n:::\n\n\n### Exemple\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](5_ARIMA_files/figure-beamer/unnamed-chunk-2-1.pdf)\n:::\n:::\n\n\n## Stationnariser une série\n### La différenciation pour stabiliser le niveau\n\n- Si la série différenciée est un bruit blanc de moyenne nulle (marche aléatoire) :\n$$\n(I-B)y_t=y_t-y_{t-1}=\\varepsilon_t \\implies y_t=y_0+\\sum_{i=1}^t\\varepsilon_i\n$$\n{{< fa arrow-circle-right >}} Modèle naïf  \nGénéralement mouvement à la hausse ou à la baisse aléatoire,\n\n. . .\n\n- Si la série différenciée est un bruit blanc de moyenne non nulle (marche aléatoire avec dérive / *drift*) :\n$$\n(I-B)y_t=c+\\varepsilon_t \\implies y_t=y_0 +ct+\\sum_{i=1}^t\\varepsilon_i\n$$\n\n. . .\n\n- Parfois on a besoin de différencier plusieurs fois $(I-B)^2y_t=(y_t-y_{t-1}) -(y_{t-1}-y_{t-2})$ ou de faire une différenciation saisonnière $(I-B^m)y_t=y_t-y_{m}$\n\n. . .\n\n- Si saisonnalité importante, commencer par la différenciation saisonnière\n\n\n\n### Modèles Intégrés (1/3)\n\nSoit X, processus « tendance linéaire » :\n$$\nX_t=\\alpha+\\beta t + \\varepsilon_t\n$$\n\nCalculer l'espérance et la variance de la v.a. $X_t$ ?  \nX est stationnaire ?  \n\n. . .\n\nDifférence d'ordre 1 : $$(I-B)X_t = ?$$\n\nLa série obtenue est-elle stationnaire ?\n\n\nSi $X$ est un processus « tendance polynomiale d'ordre 2 », comment stationnariser la série ?\n\n\n### Modèles Intégrés (2/3)\n\nSoit X, processus « saisonnier stable  » :\n$$\nX_t=S_t+\\varepsilon_t\\quad\n\\text{avec}\n\\quad\n\\forall t,\\, S_t=S_{t+s}\n$$\n$X$ stationnaire ? \n\n. . .\n\nDifférence d'ordre 1, avec retard d'ordre $s$ :\n$$(I-B^s)X_t = ?$$\n\n\nLa série obtenue est-elle stationnaire ?\n\nSi $X_t = a+bt + S_t +\\varepsilon_t$, que donnerait cette différenciation ?\n\n\n### Modèles Intégrés (3/3)\n\nUne différenciation « simple » d'ordre $d$ supprime les tendances polynomiales d'ordre $d$ :\n$$\n(I-B)^dX_t\n$$\nUne différenciation « saisonnière » supprime aussi les tendances linéaires :\n$$(I-B^s)X_t$$\n\nUne différenciation « saisonnière » d'ordre $D$ plus grand que 1 est rare :\n$$(I-B^s)^DX_t$$\n\n\n### Exemple\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(co2)\n```\n\n::: {.cell-output-display}\n![](5_ARIMA_files/figure-beamer/unnamed-chunk-3-1.pdf)\n:::\n:::\n\n\n### Exemple\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(diff(co2, 12))\n```\n\n::: {.cell-output-display}\n![](5_ARIMA_files/figure-beamer/unnamed-chunk-4-1.pdf)\n:::\n:::\n\n\n### Exemple\n\n\n::: {.cell}\n\n```{.r .cell-code}\nautoplot(diff(diff(co2, 12), 1))\n```\n\n::: {.cell-output-display}\n![](5_ARIMA_files/figure-beamer/unnamed-chunk-5-1.pdf)\n:::\n:::\n\n\n### Faut-il toujours différencier ?\n\nPour modéliser une série avec tendance on peut distinguer deux types de non-stationnarité :\n\n1. Modèle trend-stationnaire :\n$$\nX_t=a+bt+\\varepsilon_t\n$$\n2. Modèle avec racine unité\n$$\n(1-B)Y_t=b+\\eta\\implies Y_t=a+bt+\\underbrace{\\sum_{i=1}^t\\eta_t}_{\\text{tend. stochastique}}\n$$\n \nOn a $\\mathbb V [X_t]=\\mathbb V[\\varepsilon_t]=cst$ indépendante du temps mais $\\mathbb V [Y_t] = t\\mathbb V[\\eta_t]$\n\n### Exemple\n\n\\footnotesize\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1); e = rnorm(100)\nu1 = ts(cumsum(e), start = 2000, frequency = 12)\nu2 = ts(e + seq_along(e)/50,  start = 2000, frequency = 12)\nm1 = Arima(u1, order = c(0, 1, 0))\nm2 = Arima(u2, include.drift = TRUE)\n(autoplot(u1, y = NULL,main = \"Marche aléatoire\") +\n\tautolayer(forecast(m1, h = 12))) /\n(autoplot(u2, y = NULL,main = \"Trend-stationnaire\") +\n\tautolayer(forecast(m2, h = 12)))\n```\n\n::: {.cell-output-display}\n![](5_ARIMA_files/figure-beamer/unnamed-chunk-6-1.pdf)\n:::\n:::\n\n\n## Tests\n\n### Tests de racine unitaire\n\nPlusieurs tests existent pour déterminer l'ordre de différenciation :\n\n- Sur séries non-saisonnières :\n\n\t- Test de Dickey-Fuller augmenté (ADF) `fUnitRoots::adfTest()`, `tseries::adf.test()`, `urca::ur.df()`. $(H_0)$ racine unitaire (avec ou non tendance linéaire)\n\t\n\t- Test de Phillips-Perron `tseries::pp.test`, `urca::ur.pp` ou `feasts::unitroot_pp()`. $(H_0)$ racine unitaire (avec ou non tendance linéaire)\n\t\n\t- Test KPSS `tseries::kpss.test()`ou `urca::ur.kpss()`. $(H_0)$ série stationnaire (avec ou non tendance linéaire)\n\t\n- Pour les séries saisonnières : d'autres tests du type Canova-Hansen (`uroot::ch.test`), F-Tests, etc.\n\n. . .\n\n`forecast::ndiffs()` (ou `feasts::unitroot_ndiffs`) et  `forecast::nsdiffs()` (ou `feasts::unitroot_nsdiffs`) permettent de déterminer les ordres de différenciation en utilisant ces tests.\n\n### Exemple {.allowframebreaks}\n\n\\footnotesize\n\n\n::: {.cell}\n\n```{.r .cell-code}\nuroot::ch.test(co2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tCanova and Hansen test for seasonal stability\n\ndata:  co2\n\n      statistic pvalue    \nJan      2.3112      0 ***\nFeb      2.3104      0 ***\nMar      2.3137      0 ***\nApr      2.3156      0 ***\nMay      2.3158      0 ***\nJun      2.3152      0 ***\nJul      2.3142      0 ***\nAug      2.3155      0 ***\nSep      2.3128      0 ***\nOct      2.3135      0 ***\nNov      2.3136      0 ***\nDec      2.3148      0 ***\njoint    2.9814 0.0476 *  \n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTest type: seasonal dummies \nNW covariance matrix lag order: 18 \nFirst order lag: no \nOther regressors: no  \nP-values: based on response surface regressions \n```\n:::\n\n```{.r .cell-code}\nuroot::ch.test(diff(co2, 12))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tCanova and Hansen test for seasonal stability\n\ndata:  diff(co2, 12)\n\n      statistic pvalue   \nJan      0.8938  0.003 **\nFeb      0.7737 0.0067 **\nMar      0.5276 0.0327 * \nApr      0.6513 0.0149 * \nMay      0.6378 0.0162 * \nJun      0.6103 0.0193 * \nJul      0.5519 0.0281 * \nAug      0.5907 0.0219 * \nSep      0.6174 0.0185 * \nOct      0.7576 0.0075 **\nNov      0.8991 0.0029 **\nDec      1.0098 0.0014 **\njoint    1.3838 0.5406   \n---\nSignif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n\nTest type: seasonal dummies \nNW covariance matrix lag order: 18 \nFirst order lag: no \nOther regressors: no  \nP-values: based on response surface regressions \n```\n:::\n\n```{.r .cell-code}\nforecast::nsdiffs(co2) # Autre test que Canova-Hansen est utilisé\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\ntseries::kpss.test(diff(co2, 12))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tKPSS Test for Level Stationarity\n\ndata:  diff(co2, 12)\nKPSS Level = 1.9439, Truncation lag parameter = 5, p-value = 0.01\n```\n:::\n\n```{.r .cell-code}\ntseries::kpss.test(diff(diff(co2, 12), 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tKPSS Test for Level Stationarity\n\ndata:  diff(diff(co2, 12), 1)\nKPSS Level = 0.011476, Truncation lag parameter = 5, p-value = 0.1\n```\n:::\n\n```{.r .cell-code}\nforecast::ndiffs(diff(co2, 12))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\n\n# Construction du modèle ARIMA\n\n### La partie « modélisation ARIMA »\n\nARIMA, modèle auto-projectif :\n$$\nX_t = f(X_{t-1}, X_{t-2}, X_{t-3},\\, \\dots,\n\\varepsilon_{t}, \\varepsilon_{t-1}, \\varepsilon_{t-2} \\,\\dots\n)\n$$\nTrouver $f$ ?\n\nSous hypothèse de stationnarité, il existe un « modèle ARMA » qui approche la série.\n\nConséquence (th de Wold) : erreurs de prévision se comportent comme le résidu du modèle (bruit blanc)\n\nOn privilégie les modèles avec faible nombre de paramètres.\n\nMéthode de Box et Jenkins pour estimer et juger de la qualité des modèles.\n\n## Modèles AR et MA\n### Modèles Autorégressifs (AR) \n\n:::::{.colums}\n\n:::{.colum width=30%}\nModèle *autorégressif* d'ordre $p$, $AR(p)$ :\n:::\n:::{.colum width=60%}\n\\vspace{-0.5cm}\n\\begin{align*}\n&X_t = \\phi_1X_{t-1}+\\phi_2 X_{t-2} + \\dots + \\phi_p X_{t-p} + \\varepsilon_t \\\\\n\\iff& (1 -\\phi_1 B-\\phi_2 B^2 - \\dots - \\phi_p B^p ) X_t = \\varepsilon_t \\\\\n\\iff& \\Phi(B)X_t = \\varepsilon_t\n\\end{align*}\n:::\n:::::\n\n. . .\n\nOn retrouve les marches aléatoires (sans ou avec dérive)\n\n. . .\n\nUn AR modélise l'influence des $p$ réalisations passées sur la réalisation courante : effet mémoire\n\n. . .\n\nExemples classiques (voir TP)\n\n- Le niveau du lac Huron peut être modélisé par un $AR(1)$ ou un $AR(2)$ ;  \n- $AR(2)$ : nombre de tâches solaires - Yules\n\n### Exemples : que dire sur $\\phi_1$ ?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](5_ARIMA_files/figure-beamer/ar-simuls-1.pdf)\n:::\n:::\n\n\n### Exemples : réponse\n\nDans les graphiques de droite on observe une alternance entre périodes positives et négatives : $\\phi_1<0$\n\n\\footnotesize\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(100)\nar1_pos = arima.sim(n=600, list(ar=0.8))\nar1_neg = arima.sim(n=600, list(ar=-0.8))\nar2_pos = arima.sim(n=600, list(ar=c(0.3, 0.2)))\nar2_neg = arima.sim(n=600, list(ar=c(-0.3, 0.2)))\n(autoplot(ar1_pos, main = \"AR(1)\", y = NULL) + \n \tautoplot(ar1_neg, main = \"AR(1)\", y = NULL)) /\n\t(autoplot(ar2_pos, main = \"AR(2)\", y = NULL) + \n\t \tautoplot(ar2_neg, main = \"AR(2)\", y = NULL)\n\t \t)\n```\n:::\n\n\n### Reconnaitre un modèle $AR(p)$\n\nPour reconnaître un $AR(p)$ on peut analyser l'autocorrélogramme partiel (PACF) : $r(k)$ mesure relation entre $y_t$ et $y_{t-k}$ en enlevant les effets de $y_{t-1},\\dots y_{t-k-1}$  \n$\\alpha_k$ est le coefficient $\\phi_k$ dans la régression\n$$\ny_t=c+\\phi_1y_{t-1}+\\dots+\\phi_k y_{t-k}+\\varepsilon_t\n$$\n\n\nOn a $r(1)=\\rho(1)$\n\n\nPour un $AR(p)$ :\n\n- **ACF** : $\\rho(h)$ décroit exponentiellement vers 0 (ou de manière sinusoïdale si $\\phi_1<0$)  \n- **PACF** : $r(h)=0$ pour $h>p$ \n\n### Exemple $AR(2)$ $y_t = 0.3y_{t-1}+0.2y_{t-2} + \\varepsilon_t$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](5_ARIMA_files/figure-beamer/unnamed-chunk-9-1.pdf)\n:::\n:::\n\n\n### Conditions de stationnarité\n\nOn restreint généralement les modèles autorégressifs aux modèles stationnaires.\n \nPour cela il faut que les racines de $\\Phi$ soient en dehors du cercle unité (sinon un choc pourrait avoir un effet permanent)\n\n\n### Modèles « Moving Average » (MA) \n\n::::{.colums}\n:::{.colum width=30%}\nModèle *moyenne mobile* d'ordre $q$, $MA(q)$ :\n:::\n:::{.colum width=60%}\n\\vspace{-0.5cm}\n\\begin{align*}\nX_t \n&= \\varepsilon_t + \\theta_1\\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2} + \\dotsb + \\theta_q \\varepsilon_{t-q} \\\\\n\\iff X_t &= (1 +\\theta_1 B+\\theta_2 B^2 + \\dotsb + \\theta_q B^q ) \\varepsilon_t\\\\\n\\iff X_t &= \\Theta(B)\\varepsilon_t\n\\end{align*}\n:::\n::::\n\nRmq : dans certains logiciels/packages (e.g. `RJDemetra`) on utilise plutôt la notation $X_t = \\varepsilon_t - \\theta_1\\varepsilon_{t-1} - \\theta_2 \\varepsilon_{t-2} - \\dotsb - \\theta_q \\varepsilon_{t-q}$\n\n\nRésulte d'une accumulation non persistante de $q$ chocs indépendants\n\n### Exemples\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](5_ARIMA_files/figure-beamer/unnamed-chunk-10-1.pdf)\n:::\n:::\n\n\n### Reconnaitre un modèle $MA(q)$\n\nPour reconnaître un $MA(q)$ on peut analyser l'autocorrélogramme (ACF)\n\nPour un $MA(q)$ :\n\n- **ACF** : $\\rho(h)=0$ pour $h>q$ \n\n- **PACF** : $r(h)=0$ décroit exponentiellement vers 0\n\n\n### Exemple d'un $MA(3)$ $y_t = \\varepsilon_t + 0.8\\varepsilon_{t-1}+0.5\\varepsilon_{t-2}+0.6\\varepsilon_{t-3}$\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](5_ARIMA_files/figure-beamer/unnamed-chunk-11-1.pdf)\n:::\n:::\n\n\n\n## Modèle ARMA\n### Modèles ARMA \n\nModèles $ARMA(p,q)$ : combine $AR(p)$ et $MA(q)$, sans ou avec constante\n$$\n\\Phi(B)X_t = \\Theta(B) \\varepsilon_t\n$$\n$$\n\\Phi(B)X_t = \\mu + \\Theta(B) \\varepsilon_t\n$$\n\n\n## Modèles SARMA et modèles intégrés\n\n### Modèles SARMA\n\nModèle $SARMA(P,Q)$ : $ARMA$ avec polynôme d'ordre $s$ (4 séries trimestrielles, 12 séries mensuelles) :\n$$\n\\Phi(B^s)X_t = \\Theta(B^s)\\varepsilon_t\n\\text{ ou }\\Phi_s(B)X_t = \\Theta_s(B)\\varepsilon_t\n$$\nIntérêt :\n\n- montrer autocorrélations d'ordre $s$  \n- simplifier l'écriture par factorisation\n\n. . .\n\n$ARMA(p,q)(P,Q)$ combine parties régulière et saisonnière : $ARMA(p,q)\\times SARMA(P,Q)$. \n\nIdentique à $ARMA (p+P*s, q+Q*s)$\n\nExemple série mensuelle : $ARMA (1,1)(1,1)$ = $ARMA (13,13)$  \n\n\n### Modèles ARIMA\n\n$ARIMA(p,d,q)$ modélise les séries non stationnaires avec tendance\n$$\n\\Phi(B)(I-B)^dX_t = \\Theta(B)\\varepsilon_t\n$$\n\n$ARIMA(p,d,q)(P,D,Q)$ modélise les séries avec tendance et saisonnalité\n$$\n\\Phi(B)\\Phi_s(B)(I-B)^d(I-B^s)^DX_t = \\Theta(B)\\Theta_s(B)\\varepsilon_t\n$$\nFactorisation des polynômes en $B$ de la partie *régulière* et de la partie *saisonnière*\n\n. . .\n\nModèle le plus souvent observé : $ARIMA(0,1,1)(0,1,1)$ appelé modèle Airline (voir TP)\n\n\n# Détermination du modèle ARIMA\n## Méthode de Box-Jenkins\n\n### Méthode de Box-Jenkins \n\n1.\tStationnariser le processus :  $d$, $D$\n\n2.\tIdentifier les ordres ARMA : $p$, $P$, $q$, $Q$\n    {{< fa arrow-circle-right >}} structure d'autocorrélation de la série\n\n3.\tEstimer les coefficients ARMA\n    {{< fa arrow-circle-right >}} degré de variabilité de la structure d'autocorrélation   \n    {{< fa arrow-circle-right >}} Peut-on simplifier le modèle ?\n    \n4.\tValider le modèle\n    {{< fa arrow-circle-right >}} résidus = bruit blanc ?\n\n5.\tChoix du modèle (si plusieurs modèles valides)\n    {{< fa arrow-circle-right >}} critères d'information\n\n6.\tPrévision\n\n## Méthode générale\n\n### Méthode générale\n\n::: incremental\n- Tracer la série : transformation nécessaire ? Points atypiques ?\n- Stationnariser la série pour déterminer $D$ puis $d$ (analyse graphique et/ou tests)\n- Examen des ACF/PACF pour déterminer des ordres $P,Q$, $p,q$ plausibles\n- Sélection des modèles par minimisation AICc\n- Vérifier la qualité des résidus : si ne ressemble pas à un bruit blanc changer de modèle\n- Prévision\n:::\n\n\n\n### Méthode utilisée dans `forecast::auto.arima()` (1)\n\n1. On choisit $D$ (STL utilisé et non pas test de Canova-Hansen) puis $d$ déterminé en utilisant des tests successifs de\nKPSS.\n\n. . .\n\n2.  Sélection d'un des 4 modèles en minimisant l'AICc : $ARIMA(2,d,2)(1,D,1)$, $ARIMA(2,d,2)(0,D,0)$, $ARIMA(1,d,0)(1,D,0)$ et $ARIMA(0,d,1)(0,D,1)$\n\n. . .\n\n3.  On considère 30 variations du modèle retenu :\n\n-   En faisant varier un seul des paramètres $p$, $q$, $P$ ou $Q$ de $\\pm 1$ ;\n-   En faisant varier $p$ et $q$ en même temps de $\\pm 1$ ;\n-   En faisant varier $P$ et $Q$ en même temps de $\\pm 1$ ;\n-   En incluant ou non la constante.\n-   Si un modèle minimise l'AICc on recommence.\n\n### Méthode utilisée dans `forecast::auto.arima()` (2)\n\nSi on n'a qu'une série, utiliser  `forecast::auto.arima(., stepwise = FALSE, approximation = FALSE)` pour étudier tous les modèles (plus lent).\n\nD'autres algorithmes (TRAMO ou pickmdl) utilisent également d'autres tests : autocorrélation (Ljung-Box à l'ordre $2m$), tests de sur-différenciation, de passage au log.\n\n# ARIMA et ETS\n\n### Équivalence entre ARIMA et ETS\n\nIl y a plusieurs équivalences entre ETS et ARIMA :\n\n- Modèles exponentiels linéaires sont tous des cas particulier de modèle ARIMA\n\n- Modèles exponentiels non linéaires n'ont pas d'équivalent ARIMA\n\n- De nombreux ARIMA n'ont pas de modèle ETS équivalent\n\n- Les modèles ETS sont non-stationnaires\n\n### Exemple 1 : $ETS(A,N,N)$\n\n$$\n\\begin{cases}\ny_t&=l_{t-1}+ \\varepsilon_t \\\\\nl_t&=l_{t-1} +\\alpha \\varepsilon_t\n\\end{cases}\\implies y_t=(y_{t-1}-\\varepsilon_{t-1}+\\alpha\\varepsilon_{t-1})+\\varepsilon_t\n$$\nDonc $y_t\\sim ARIMA(0,1,1)$ :\n$$\n(1-B)y_{t}=(1+(\\alpha-1)B)\\varepsilon_t\n$$\n\n### Exemple 2 : $ETS(A,A,N)$\n\n$$\n\\begin{cases}\ny_t&=l_{t-1} + b_{t-1} +\\varepsilon_t\\\\\nl_t&=l_{t-1}+b_{t-1}+\\alpha \\varepsilon_t\\\\\nb_t&=b_{t-1}+\\beta\\varepsilon_t\n\\end{cases} \\iff\n\\begin{cases}\ny_t&=l_{t-1} + b_{t-1} +\\varepsilon_t\\\\\n(1-B)l_t&=b_{t-1}+\\alpha \\varepsilon_t\\\\\n(1-B)b_t&=\\beta\\varepsilon_t\n\\end{cases}\n$$\nD'où\n\\begin{align*}\n(1-B)^2y_t &=(1-B)B(Bb_{t}+\\alpha\\varepsilon_t)+(1-B)B\\beta\\varepsilon_t+(1-B)^2\\varepsilon_t \\\\\n&=B^2\\beta\\varepsilon_t+(1-B)B(\\beta+\\alpha)\\varepsilon_t+(1-2B+B^2)\\varepsilon_t \\\\\n&=\\left[1+(\\alpha+\\beta-2)B+(1-\\alpha)B^2\\right]\\varepsilon_t\n\\end{align*}\n\n\net $y_t\\sim ARIMA(0,2,2)$\n\n### Exemple 3 : $ETS(A,A,A)$\n\n\\footnotesize\n\n$$\n\\begin{cases}\ny_t&=l_{t-1} + b_{t-1} +s_{t-m}+\\varepsilon_t\\\\\nl_t&=l_{t-1}+b_{t-1}+\\alpha \\varepsilon_t\\\\\nb_t&=b_{t-1}+\\beta\\varepsilon_t \\\\\ns_t&=s_{t-m}+\\gamma\\varepsilon_t\n\\end{cases} \\iff\n\\begin{cases}\ny_t&=Bl_{t} + Bb_{t} +B^ms_{t}+\\varepsilon_t\\\\\n(1-B)l_{t}&=Bb_{t}+\\alpha \\varepsilon_{t}\\\\\n(1-B)b_{t}&=\\beta\\varepsilon_{t} \\\\\n(1-B^{m})s_{t}&=\\gamma\\varepsilon_{t}\n\\end{cases}\n$$\n\\begin{align*}\n(1-B)(1-B^m)y_t &=(1-B^m)B^2b_{t}+(1-B^m)B\\alpha\\varepsilon_t+(1-B^m)B\\beta\\varepsilon_t++\\\\\n&\\phantom{=+}(1-B)B^m\\gamma \\varepsilon_t + (1-B)(1-B^m)\\varepsilon_t \\\\\n&=\\big[(1+B+\\dots+B^{m-1})B^2\\beta +(1-B^m)B(\\alpha+\\beta)+\\\\\n&\\phantom{=+}(1-B)B^m(\\gamma-1)+(1-B)\\big]\\varepsilon_t\n\\end{align*}\n\nDonc $y_t\\sim ARIMA(0,1,m+1)(0,1,0)$\n\n### Comparaison des modèles ETS et ARIMA\n\nPeut-on comparer les modèles ETS et ARIMA en utilisant un critère d'information ?\n\n. . .\n\n\\bcattention NON ! Attention aux ordres de différenciation, sur un modèle ARIMA ne comparer par exemple que les modèles qui ont le même ordre de différenciation\n\n. . . \n\n... Mais quid lorsque les modèles sont équivalents : peut-on comparer $ETS(A,A,A)$ avec un $ARIMA(0,1,1)(0,1,1)$ ?\n\n. . . \n\n\\bcattention Encore NON ! Avec un modèle ARIMA, lorsqu'il y a différenciation on perd les premières données alors que pour un ETS on a des prévisions sur toutes les données.\n\n# Retour sur TRAMO-SEATS\n\n## TRAMO\n### Principe de TRAMO\n\nTRAMO = Time series Regression with ARIMA noise, Missing values and Outliers\n\n\nObjectifs de TRAMO :\n\n- corriger la série de points atypiques, des effets de calendrier et imputation des valeurs manquantes\n\n- prolonger la série\n\n- fournir à SEATS le modèle ARIMA à la base de la décomposition\n\n$$\nY_t = \\sum \\hat{\\alpha}_i O_{it} + \\sum\\hat\\beta_j C_{jt} + \\varepsilon_t\n$$\nEt on modélise $(Y_t -\\sum \\hat{\\alpha}_i O_{it} + \\sum\\hat\\beta_j C_{jt}) = \\varepsilon_t$ comme un modèle ARIMA\n\n## SEATS\n### Principe de SEATS (1/3)\nSEATS = Signal Extraction in ARIMA Time Series\n\nSEATS utilise le modèle ARIMA de la série linéarisée TRAMO : \n$$\n\\underbrace{\\Phi(B)\\Phi_s(B)(I-B)^d(I-B^s)^D}_{\\Phi(B)}X_t = \\underbrace{\\Theta(B)\\Theta_s(B)}_{\\Theta(B)}\\varepsilon_t\n$$\nHypothèses :\n\n1. La série linéarisée peut être modélisée par un modèle ARIMA\n\n2. Les différentes composantes sont décorrélées et chaque composante peut être modélisée par un modèle ARIMA\n\n3. Les polynomes AR des composantes n'ont pas de racine commune\n\n### Principe de SEATS (2/3)\nOn factorise le polynôme AR $\\Phi(B)$:\n$$\n\\Phi(B) = \\phi_T(B) \\phi_S(B) \\phi_C(B)\n$$\n\n- $\\phi_T(B)$ racines correspondant à la tendance\n\n- $\\phi_S(B)$ racines correspondant à la saisonnalité\n\n- $\\phi_C(B)$ racines correspondant au cycle\n\n\n### Principe de SEATS (3/3)\n\n$X_t$ est exprimé sous la forme :\n$$\nX_t = \\frac{\\Theta(B)}{\\Phi(B)}\\varepsilon_t =\n\\underbrace{\\frac{\\theta_T(B)}{\\phi_T(B)}\\varepsilon_{T,t}}_{\\text{Tendance}}\n+\n\\underbrace{\\frac{\\theta_S(B)}{\\phi_S(B)}\\varepsilon_{S,t}}_{\\text{Saisonnalité}}\n+\n\\underbrace{\\frac{\\theta_C(B)}{\\phi_C(B)}\\varepsilon_{C,t}}_{\\text{Cycle}}\n+ \\underbrace{\\nu_t}_{\\substack{\\text{Irrégulier}\\\\\\text{(bruit}\\\\\\text{blanc)}}}\n$$\nUn modèle ARIMA est associé à chaque composante.\n\nInfinité de solutions : on retient celle qui minimise la variance de l'irrégulier\n\n{{< fa arrow-circle-right >}} Estimation par filtre de Wiener-Kolmogorov\n\n\n# Conclusion\n### Les essentiels\n\nLes séries économiques ne sont pas stationnaires, ni leur niveau, ni leurs fluctuations ne sont constants dans le temps\n\nIntégrer un processus permet de le stationnariser\n\nUn MA capte les fluctuations non persistantes autour d'un niveau constant - processus stationnaire\n\nUn AR met en évidence l'influence des réalisations passées sur la réalisation courante\n\nUn ARIMA reflète la structure des autocorrélations de la série, ainsi que le degré de sa variabilité dans le temps\n\nL'examen des résidus permet de valider les modèles, le choix \"optimal\" se fait grâce aux critères d'information\n\n\n### Bibliographie\n\n\nHyndman, R.J., & Athanasopoulos, G. (2018) *Forecasting: principles and practice*, 2nd edition, OTexts: Melbourne, Australia. OTexts.com/fpp2. Accessed on oct. 2023.\n\nHyndman, R.J., & Athanasopoulos, G. (2021) *Forecasting: principles and practice*, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on oct. 2023.\n\n\n",
    "supporting": [
      "5_ARIMA_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}